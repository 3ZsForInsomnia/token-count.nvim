local M = {}

--- Model configuration mapping
--- @type table<string, {name: string, provider: string, encoding: string, context_window: number, tokencost_name: string, technical_name: string, max_output_tokens: number}>
M.models = {
	-- OpenAI Models (accurate with tiktoken via tokencost)
	["gpt-4"] = {
		name = "GPT-4",
		provider = "tiktoken",
		encoding = "cl100k_base",
		context_window = 8192,
		max_output_tokens = 4096,
		tokencost_name = "gpt-4",
		technical_name = "gpt-4",
	},
	["gpt-4-32k"] = {
		name = "GPT-4 32K",
		provider = "tiktoken",
		encoding = "cl100k_base",
		context_window = 32768,
		max_output_tokens = 4096,
		tokencost_name = "gpt-4-32k",
		technical_name = "gpt-4-32k",
	},
	["gpt-4-turbo"] = {
		name = "GPT-4 Turbo",
		provider = "tiktoken",
		encoding = "cl100k_base",
		context_window = 128000,
		max_output_tokens = 4096,
		tokencost_name = "gpt-4-turbo",
		technical_name = "gpt-4-turbo",
	},
	["gpt-4o"] = {
		name = "GPT-4o",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 128000,
		max_output_tokens = 16384,
		tokencost_name = "gpt-4o",
		technical_name = "gpt-4o",
	},
	["gpt-4o-mini"] = {
		name = "GPT-4o Mini",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 128000,
		max_output_tokens = 16384,
		tokencost_name = "gpt-4o-mini",
		technical_name = "gpt-4o-mini",
	},
	["gpt-3.5-turbo"] = {
		name = "GPT-3.5 Turbo",
		provider = "tiktoken",
		encoding = "cl100k_base",
		context_window = 16385,
		max_output_tokens = 4096,
		tokencost_name = "gpt-3.5-turbo",
		technical_name = "gpt-3.5-turbo",
	},
	["o1-preview"] = {
		name = "OpenAI o1 Preview",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 128000,
		max_output_tokens = 32768,
		tokencost_name = "o1-preview",
		technical_name = "o1-preview",
	},
	["o1-mini"] = {
		name = "OpenAI o1 Mini",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 128000,
		max_output_tokens = 65536,
		tokencost_name = "o1-mini",
		technical_name = "o1-mini",
	},
	["gpt-5"] = {
		name = "GPT-5",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 400000,
		max_output_tokens = 128000,
		tokencost_name = "gpt-5",
		technical_name = "gpt-5",
	},
	["gpt-5-mini"] = {
		name = "GPT-5 Mini",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 400000,
		max_output_tokens = 128000,
		tokencost_name = "gpt-5-mini",
		technical_name = "gpt-5-mini",
	},
	["gpt-5-nano"] = {
		name = "GPT-5 Nano",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 400000,
		max_output_tokens = 128000,
		tokencost_name = "gpt-5-nano",
		technical_name = "gpt-5-nano",
	},

	-- Additional OpenAI models from tiktoken mappings
	["gpt-4.1"] = {
		name = "GPT-4.1",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 1047576,
		max_output_tokens = 32768,
		tokencost_name = "gpt-4.1",
		technical_name = "gpt-4.1",
	},
	["gpt-4.1-mini"] = {
		name = "GPT-4.1 Mini",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 1047576,
		max_output_tokens = 32768,
		tokencost_name = "gpt-4.1-mini",
		technical_name = "gpt-4.1-mini",
	},
	["gpt-4.1-nano"] = {
		name = "GPT-4.1 Nano",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 1047576,
		max_output_tokens = 32768,
		tokencost_name = "gpt-4.1-nano",
		technical_name = "gpt-4.1-nano",
	},
	["chatgpt-4o-latest"] = {
		name = "ChatGPT-4o Latest",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 128000,
		max_output_tokens = 4096,
		tokencost_name = "chatgpt-4o-latest",
		technical_name = "chatgpt-4o-latest",
	},
	["o3"] = {
		name = "OpenAI o3",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 200000,
		max_output_tokens = 100000,
		tokencost_name = "o3",
		technical_name = "o3",
	},
	["o4-mini"] = {
		name = "OpenAI o4 Mini",
		provider = "tiktoken",
		encoding = "o200k_base",
		context_window = 200000,
		max_output_tokens = 100000,
		tokencost_name = "o4-mini",
		technical_name = "o4-mini",
	},

	-- Anthropic Models (estimates via tokencost, accurate if official API enabled)
	["claude-3-haiku"] = {
		name = "Claude 3 Haiku",
		provider = "tokencost",
		encoding = "claude-3-haiku-20240307",
		context_window = 200000,
		max_output_tokens = 4096,
		tokencost_name = "claude-3-haiku-20240307",
		technical_name = "claude-3-haiku",
	},
	["claude-3-sonnet"] = {
		name = "Claude 3 Sonnet",
		provider = "tokencost",
		encoding = "claude-3-sonnet-20240229",
		context_window = 200000,
		max_output_tokens = 4096,
		tokencost_name = "claude-3-sonnet-20240229",
		technical_name = "claude-3-sonnet",
	},
	["claude-3-opus"] = {
		name = "Claude 3 Opus",
		provider = "tokencost",
		encoding = "claude-3-opus-20240229",
		context_window = 200000,
		max_output_tokens = 4096,
		tokencost_name = "claude-3-opus-20240229",
		technical_name = "claude-3-opus",
	},
	["claude-3.5-sonnet"] = {
		name = "Claude 3.5 Sonnet",
		provider = "tokencost",
		encoding = "claude-3-5-sonnet-20241022",
		context_window = 200000,
		max_output_tokens = 8192,
		tokencost_name = "claude-3-5-sonnet-20241022",
		technical_name = "claude-3.5-sonnet",
	},
	["claude-3.5-haiku"] = {
		name = "Claude 3.5 Haiku",
		provider = "tokencost",
		encoding = "claude-3-5-haiku-20241022",
		context_window = 200000,
		max_output_tokens = 8192,
		tokencost_name = "claude-3-5-haiku-20241022",
		technical_name = "claude-3.5-haiku",
	},
	["claude-4-sonnet"] = {
		name = "Claude 4 Sonnet",
		provider = "tokencost",
		encoding = "claude-4-sonnet-20250514",
		context_window = 1000000,
		max_output_tokens = 1000000,
		tokencost_name = "claude-4-sonnet-20250514",
		technical_name = "claude-4-sonnet",
	},
	["claude-4-opus"] = {
		name = "Claude 4 Opus",
		provider = "tokencost",
		encoding = "claude-4-opus-20250514",
		context_window = 200000,
		max_output_tokens = 32000,
		tokencost_name = "claude-4-opus-20250514",
		technical_name = "claude-4-opus",
	},
	["claude-4.5-sonnet"] = {
		name = "Claude Sonnet 4.5",
		provider = "tokencost",
		encoding = "claude-4-sonnet-20250514",
		context_window = 200000,
		max_output_tokens = 200000,
		tokencost_name = "claude-4.5-sonnet-20250514",
		technical_name = "claude-4.5-sonnet",
	},

	-- Google Gemini Models (estimates via tokencost, accurate if official API enabled)
	["gemini-2.0-flash"] = {
		name = "Gemini 2.0 Flash",
		provider = "tokencost",
		encoding = "gemini-2.0-flash-001",
		context_window = 1048576,
		max_output_tokens = 8192,
		tokencost_name = "gemini-2.0-flash-001",
		technical_name = "gemini-2.0-flash",
	},
	["gemini-1.5-pro"] = {
		name = "Gemini 1.5 Pro",
		provider = "tokencost",
		encoding = "gemini-1.5-pro-002",
		context_window = 2097152,
		max_output_tokens = 8192,
		tokencost_name = "gemini-1.5-pro-002",
		technical_name = "gemini-1.5-pro",
	},
	["gemini-1.5-flash"] = {
		name = "Gemini 1.5 Flash",
		provider = "tokencost",
		encoding = "gemini-1.5-flash-002",
		context_window = 1048576,
		max_output_tokens = 8192,
		tokencost_name = "gemini-1.5-flash-002",
		technical_name = "gemini-1.5-flash",
	},
	["gemini-pro"] = {
		name = "Gemini Pro",
		provider = "tokencost",
		encoding = "gemini-1.0-pro",
		context_window = 32760,
		max_output_tokens = 8192,
		tokencost_name = "gemini-1.0-pro",
		technical_name = "gemini-pro",
	},

	-- DeepSeek Models (accurate with official tokenizer)
	["deepseek-chat"] = {
		name = "DeepSeek Chat",
		provider = "deepseek",
		encoding = "deepseek-chat",
		context_window = 128000,
		max_output_tokens = 4096,
		tokencost_name = "deepseek-chat",
		technical_name = "deepseek-chat",
	},
	["deepseek-coder"] = {
		name = "DeepSeek Coder",
		provider = "deepseek",
		encoding = "deepseek-coder",
		context_window = 128000,
		max_output_tokens = 4096,
		tokencost_name = "deepseek-coder",
		technical_name = "deepseek-coder",
	},
	["deepseek-r1"] = {
		name = "DeepSeek R1",
		provider = "tokencost",
		encoding = "deepseek/deepseek-reasoner",
		context_window = 65536,
		max_output_tokens = 8192,
		tokencost_name = "deepseek/deepseek-reasoner",
		technical_name = "deepseek-r1",
	},
	["deepseek-v3"] = {
		name = "DeepSeek V3",
		provider = "tokencost",
		encoding = "deepseek/deepseek-v3",
		context_window = 65536,
		max_output_tokens = 8192,
		tokencost_name = "deepseek/deepseek-v3",
		technical_name = "deepseek-v3",
	},

	-- Meta Llama Models (estimates via tokencost)
	["llama-3.1-405b"] = {
		name = "Llama 3.1 405B",
		provider = "tokencost",
		encoding = "meta.llama3-1-405b-instruct-v1:0",
		context_window = 128000,
		max_output_tokens = 4096,
		tokencost_name = "meta.llama3-1-405b-instruct-v1:0",
		technical_name = "llama-3.1-405b",
	},
	["llama-3.1-70b"] = {
		name = "Llama 3.1 70B",
		provider = "tokencost",
		encoding = "meta.llama3-1-70b-instruct-v1:0",
		context_window = 128000,
		max_output_tokens = 2048,
		tokencost_name = "meta.llama3-1-70b-instruct-v1:0",
		technical_name = "llama-3.1-70b",
	},
	["llama-3.1-8b"] = {
		name = "Llama 3.1 8B",
		provider = "tokencost",
		encoding = "meta.llama3-1-8b-instruct-v1:0",
		context_window = 128000,
		max_output_tokens = 2048,
		tokencost_name = "meta.llama3-1-8b-instruct-v1:0",
		technical_name = "llama-3.1-8b",
	},
	["llama-3.3-70b"] = {
		name = "Llama 3.3 70B",
		provider = "tokencost",
		encoding = "meta.llama3-3-70b-instruct-v1:0",
		context_window = 128000,
		max_output_tokens = 4096,
		tokencost_name = "meta.llama3-3-70b-instruct-v1:0",
		technical_name = "llama-3.3-70b",
	},

	-- xAI Grok Models (estimates via tokencost)
	["grok-beta"] = {
		name = "Grok Beta",
		provider = "tokencost",
		encoding = "xai/grok-beta",
		context_window = 131072,
		max_output_tokens = 131072,
		tokencost_name = "xai/grok-beta",
		technical_name = "grok-beta",
	},
	["grok-4"] = {
		name = "Grok 4",
		provider = "tokencost",
		encoding = "xai/grok-4",
		context_window = 256000,
		max_output_tokens = 256000,
		tokencost_name = "xai/grok-4",
		technical_name = "grok-4",
	},

	-- Mistral Models (estimates via tokencost)
	["mistral-large"] = {
		name = "Mistral Large",
		provider = "tokencost",
		encoding = "mistral/mistral-large-2411",
		context_window = 128000,
		max_output_tokens = 128000,
		tokencost_name = "mistral/mistral-large-2411",
		technical_name = "mistral-large",
	},
	["mistral-small"] = {
		name = "Mistral Small",
		provider = "tokencost",
		encoding = "mistral/mistral-small-2409",
		context_window = 32000,
		max_output_tokens = 8191,
		tokencost_name = "mistral/mistral-small-2409",
		technical_name = "mistral-small",
	},
	["codestral"] = {
		name = "Codestral",
		provider = "tokencost",
		encoding = "mistral/codestral-latest",
		context_window = 32000,
		max_output_tokens = 8191,
		tokencost_name = "mistral/codestral-latest",
		technical_name = "codestral",
	},

	-- Perplexity Models (estimates via tokencost)
	["perplexity-sonar-small"] = {
		name = "Perplexity Sonar Small",
		provider = "tokencost",
		encoding = "perplexity/llama-3.1-sonar-small-128k-online",
		context_window = 127072,
		max_output_tokens = 127072,
		tokencost_name = "perplexity/llama-3.1-sonar-small-128k-online",
		technical_name = "perplexity-sonar-small",
	},
	["perplexity-sonar-large"] = {
		name = "Perplexity Sonar Large",
		provider = "tokencost",
		encoding = "perplexity/llama-3.1-sonar-large-128k-online",
		context_window = 127072,
		max_output_tokens = 127072,
		tokencost_name = "perplexity/llama-3.1-sonar-large-128k-online",
		technical_name = "perplexity-sonar-large",
	},

	-- Cohere Models (estimates via tokencost)
	["command-r-plus"] = {
		name = "Command R+",
		provider = "tokencost",
		encoding = "command-r-plus",
		context_window = 128000,
		max_output_tokens = 4096,
		tokencost_name = "command-r-plus",
		technical_name = "command-r-plus",
	},
	["command-r"] = {
		name = "Command R",
		provider = "tokencost",
		encoding = "command-r",
		context_window = 128000,
		max_output_tokens = 4096,
		tokencost_name = "command-r",
		technical_name = "command-r",
	},

	-- GitHub Copilot (approximated with tiktoken)
	["github-copilot"] = {
		name = "GitHub Copilot",
		provider = "tokencost",
		encoding = "gpt-4",
		context_window = 8192,
		max_output_tokens = 4096,
		tokencost_name = "gpt-4",
		technical_name = "github-copilot",
	},

	-- Generic fallback option (default)
	["generic"] = {
		name = "Generic (GPT-4 compatible)",
		provider = "tokencost",
		encoding = "gpt-4",
		context_window = 8192,
		max_output_tokens = 4096,
		tokencost_name = "gpt-4",
		technical_name = "generic",
	},
}

return M
